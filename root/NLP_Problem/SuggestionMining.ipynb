{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing generic modules\n",
    "import csv\n",
    "import textacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = False #set true to train all models\n",
    "EVAL = not TRAIN\n",
    "ModelList = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*DATA LOADING AND PRE-PROCESSING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "data = []\n",
    "with open('/home/aniket/Subtask-A/V1.4_Training.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        data.append({})\n",
    "        data[-1]['paper_id'] = row[0]\n",
    "        data[-1]['text'] = row[1]\n",
    "        data[-1]['label'] = row[2]\n",
    "        if int(data[-1]['label']) == 1:\n",
    "            pos+= 1\n",
    "        else:\n",
    "            neg+= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#preprocessing data\n",
    "def preprocess(data):\n",
    "    for instance in data:\n",
    "        text = instance['text']\n",
    "        text = textacy.preprocess_text(text, fix_unicode=True,\n",
    "                                            lowercase=True,\n",
    "                                            no_urls=True,\n",
    "                                            no_emails=True,\n",
    "                                            no_phone_numbers=True,\n",
    "                                            no_numbers=True,\n",
    "                                            no_currency_symbols=True,\n",
    "                                            no_punct=False,\n",
    "                                            no_contractions=True,\n",
    "                                            no_accents=True)\n",
    "        text = text.replace('\\n', ' ').strip('\\'').strip('\"')\n",
    "        text = text.replace('___', ' ')\n",
    "        text = text.replace('-', ' ')\n",
    "        text = text.replace('__', ' ')\n",
    "        text = text.replace('\"\"\"\"\"\"', '\"')\n",
    "        text = text.replace('\"\"\"\"\"', '\"')\n",
    "        text = text.replace('\"\"\"\"', '\"')\n",
    "        text = text.replace('\"\"\"', '\"')\n",
    "        text = text.replace(',', '')\n",
    "        instance['text'] = text\n",
    "        instance['tokenized_text'] = word_tokenize(text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation data\n",
    "val_data = []\n",
    "with open('/home/aniket/Subtask-A/SubtaskA_Trial_Test_Labeled.csv', encoding = 'ISO-8859-1') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        val_data.append({})\n",
    "        val_data[-1]['paper_id'] = row[0]\n",
    "        val_data[-1]['text'] = row[1]\n",
    "        val_data[-1]['label'] = row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final test data\n",
    "test_data = []\n",
    "test_data_csv = []\n",
    "with open('/home/aniket/Subtask-A/SubtaskA_EvaluationData.csv', encoding = 'ISO-8859-1') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    for i, row in enumerate(csv_reader):\n",
    "        test_data.append({})\n",
    "        test_data_csv.append([row[0], row[1], row[2]])\n",
    "        test_data[-1]['paper_id'] = row[0]\n",
    "        test_data[-1]['text'] = row[1]\n",
    "        test_data[-1]['label'] = row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#driver for preprocessing validation and train data \n",
    "from random import shuffle\n",
    "data = preprocess(data)\n",
    "val_data = preprocess(val_data)\n",
    "test = preprocess(test_data)\n",
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deep Learning**\n",
    "\n",
    "My proposed approach is to use a pretrained model. I use BERT-base(12 layers and 768 features). But, only using a pretrained model yeilds an f1-score of about 0.75 on validation data. Henceforth, all f1-scores reprted will be on validation data. I implemented some models in which the BERT was finetuned and some in which the BERT weights are freezed. I added other architectural units (LSTM, CNN) to BERT and the best f1-score I observed was 0.84-0.85. The approach of model-3 gives this score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-pretrained-bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "#install ml specific modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from pytorch_pretrained_bert.optimization import BertAdam\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch data\n",
    "def batchify(data, batch_size = 16):\n",
    "    batched_text = []\n",
    "    batched_labels = []\n",
    "    cur_batch_text = []\n",
    "    cur_batch_labels = []\n",
    "    num_done = 0\n",
    "    for i, d in enumerate(data):\n",
    "        if i % batch_size == 0 and i != 0:\n",
    "            batched_text.append(cur_batch_text)\n",
    "            batched_labels.append(cur_batch_labels)\n",
    "            cur_batch_labels = []\n",
    "            cur_batch_text = []\n",
    "            num_done = i\n",
    "        cur_batch_text.append(d['text'])\n",
    "        try:\n",
    "            cur_batch_labels.append(int(d['label']))\n",
    "        except:\n",
    "            cur_batch_labels.append(-1)\n",
    "            \n",
    "    if num_done < len(data):\n",
    "        batched_text.append(cur_batch_text)\n",
    "        batched_labels.append(cur_batch_labels)\n",
    "    return batched_text, batched_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training loop for all models\n",
    "def train(model, optimizer, data, val_data, out_file):\n",
    "    #getting data as batches\n",
    "    batched_text, batched_labels = batchify(data)\n",
    "    val_batched_text, val_batched_labels = batchify(val_data)\n",
    "    performance = []\n",
    "    #training\n",
    "    for epoch in range(30):\n",
    "        train_loss = 0\n",
    "        train_preds = []\n",
    "        train_targets = []\n",
    "        performance.append({})\n",
    "        #looping through the training set\n",
    "        for i in tqdm(range(len(batched_labels))):\n",
    "            x = batched_text[i]\n",
    "            y = batched_labels[i]\n",
    "            preds = model(x)\n",
    "            loss = model.loss(preds, y)\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_preds.append(preds)\n",
    "            train_targets.append(y)\n",
    "            train_loss = train_loss + loss.data\n",
    "        train_preds = [k[i] for k in train_preds for i in range(k.shape[0])]\n",
    "        train_targets = [i for item in train_targets for i in item]\n",
    "        eval = model.eval(train_preds, train_targets)\n",
    "        train_loss = train_loss / len(batched_text)\n",
    "        print('epoch:'+str(epoch)+' loss:'+str(train_loss)+' precision:'+str(eval[0])+\n",
    "              ' recall:'+str(eval[1])+' f1:'+str(eval[2]))\n",
    "\n",
    "        test_preds = []\n",
    "        test_targets = []\n",
    "        #looping through the val data\n",
    "        for i in range(len(val_batched_labels)):\n",
    "            x = val_batched_text[i]\n",
    "            y = val_batched_labels[i]\n",
    "            preds = model(x)\n",
    "            test_preds.append(preds.data.cpu().numpy())\n",
    "            test_targets.append(y)\n",
    "            \n",
    "        test_preds = [k[i] for k in test_preds for i in range(k.shape[0])]\n",
    "        test_targets = [i for item in test_targets for i in item]\n",
    "        test_eval = model.eval(test_preds, test_targets)\n",
    "        print('epoch:'+str(epoch)+' precision:'+str(test_eval[0])+' recall:'+\n",
    "              str(test_eval[1])+' f1:'+str(test_eval[2]))\n",
    "        \n",
    "        #storing the details\n",
    "        performance[-1]['epoch'] = epoch\n",
    "        performance[-1]['loss'] = loss\n",
    "        performance[-1]['train_precision'] = eval[0]\n",
    "        performance[-1]['train_recall'] = eval[1]\n",
    "        performance[-1]['train_f1'] = eval[2]\n",
    "        performance[-1]['val_precision'] = test_eval[0]\n",
    "        performance[-1]['val_recall'] = test_eval[1]\n",
    "        performance[-1]['val_f1'] = test_eval[2]\n",
    "        torch.save(model.state_dict(), 'models/'+out_file+'.bin')\n",
    "        torch.save(bertoptimizer.state_dict(), 'optimizer/'+out_file+'.bin')\n",
    "        with open('performance/'+out_file+'.pkl', 'wb') as f:\n",
    "            pickle.dump(performance, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL-1**\n",
    "\n",
    "*BERT-FINETUNE*\n",
    "\n",
    "In this model, I am loading the pretrained bert-base model. I average the features of the final layer of the BERT model and pass them through a fully connected layer for classification. This model achieves an f1-score of 0.75. The bert weights are finetuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class suggestionmodelBERTFinetune(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(suggestionmodelBERTFinetune, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('/home/aniket/Downloads/bert-base-uncased.tar.gz').cuda()\n",
    "        self.bert_model.train()\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.loss_fn = nn.BCELoss(reduce = True)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        bert_avg_representaions = []\n",
    "        for instance in text:\n",
    "            #preparing text to be input to BERT model\n",
    "            instance = '[CLS] '+instance+' [SEP]'\n",
    "            tokenized_text = self.tokenizer.tokenize(instance)\n",
    "            if len(tokenized_text) > 512:\n",
    "                tokenized_text = tokenized_text[:512]\n",
    "            \n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens]).long().cuda()\n",
    "            segment_ids = torch.zeros(tokens_tensor.size()).long().cuda()\n",
    "            #the bert model return a list of size 12 (12 layers), each of size (1, seq_length, 768)\n",
    "            encoder_layers, _ = self.bert_model(tokens_tensor, segment_ids)\n",
    "            #getting the features of the last layer\n",
    "            bert_representation = encoder_layers[-1]\n",
    "            bert_representation = torch.mean(bert_representation, dim = 1)\n",
    "            bert_avg_representaions.append(bert_representation)\n",
    "        \n",
    "        representation = torch.stack(bert_avg_representaions)\n",
    "        prediction = F.sigmoid(self.linear(representation))\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, preds, y):\n",
    "        #loss calculation\n",
    "        y = torch.tensor(y).float().cuda()\n",
    "        preds = preds.reshape(-1)\n",
    "        return self.loss_fn(preds, y)\n",
    "    \n",
    "    def eval(self, preds, y):\n",
    "        #evaluation of the model predictions\n",
    "        assert len(preds) == len(y)\n",
    "        z = np.zeros(len(preds))\n",
    "        for i,p in enumerate(preds):\n",
    "            if p>0.5:\n",
    "                z[i] = 1\n",
    "        prec_score = precision_score(np.array(y), z)\n",
    "        rec_score = recall_score(np.array(y), z)\n",
    "        f1_score = (2 * prec_score * rec_score)/(prec_score + rec_score)\n",
    "        return (prec_score, rec_score, f1_score)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*MODEL CREATION AND TRAINING*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.local/lib/python3.5/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model1 = suggestionmodelBERTFinetune().cuda()\n",
    "bertoptimizer1 = BertAdam(filter(lambda p: p.requires_grad,\n",
    "                                model1.parameters()), lr = 3e-5)\n",
    "#training\n",
    "if TRAIN:\n",
    "    train(model1, bertoptimizer1, data, val_data, 'BertFinetune')\n",
    "if EVAL:\n",
    "    model1.load_state_dict(torch.load('models/BertFinetune.bin'))\n",
    "    ModelList.append({'name':'BertFinetune', 'model':model1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL-2**\n",
    "\n",
    "*BERT + side-way LSTM*\n",
    "\n",
    "This model considers all 12 layers of the bert model for classification as opposed to the previous model which considered only the last layer. The outputs of the 12 layers are passed through an lstm of sequence length 12. The batch size for this LSTM is the sequence length of the sentence. This combination of layers was inspired by https://arxiv.org/pdf/1903.05987.pdf. This paper used a weighted combination of all layers. I chose to use an LSTM to get a combined representation of all layers. I refer to this lstm as a **side-way lstm**. This model obtained an f1-score of 0.82. The bert model is fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class suggestionmodelBERTFinetuneSidewayLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(suggestionmodelBERTFinetuneSidewayLSTM, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('/home/aniket/Downloads/bert-base-uncased.tar.gz').cuda()\n",
    "        self.bert_model.train()\n",
    "        self.lstm = nn.LSTM(768, 256)\n",
    "        self.linear = nn.Linear(256, 1)\n",
    "        self.loss_fn = nn.BCELoss(reduce = True)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        bert_avg_representaions = []\n",
    "        for instance in text:\n",
    "            instance = '[CLS] '+instance+' [SEP]'\n",
    "            tokenized_text = self.tokenizer.tokenize(instance)\n",
    "            if len(tokenized_text) > 512:\n",
    "                tokenized_text = tokenized_text[:512]\n",
    "            \n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens]).long().cuda()\n",
    "            segment_ids = torch.zeros(tokens_tensor.size()).long().cuda()\n",
    "            encoder_layers, _ = self.bert_model(tokens_tensor, segment_ids)\n",
    "            bert_representation = torch.squeeze(torch.stack(encoder_layers))\n",
    "            #side-way lstm\n",
    "            _, (bert_lstm_representation, _) = self.lstm(bert_representation)\n",
    "            #taking max across the side-way lstm representations of all words in the sentence\n",
    "            bert_representation = torch.max(torch.squeeze(bert_lstm_representation), dim = 0)[0]\n",
    "            bert_avg_representaions.append(bert_representation)\n",
    "        \n",
    "        representation = torch.stack(bert_avg_representaions)\n",
    "        prediction = F.sigmoid(self.linear(representation))\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, preds, y):\n",
    "        y = torch.tensor(y).float().cuda()\n",
    "        preds = preds.reshape(-1)\n",
    "        entropy = preds * torch.log(preds) + (1 - preds) * torch.log(preds)\n",
    "        entropy = - torch.sum(entropy)\n",
    "        loss = self.loss_fn(preds, y) + 0.05 * entropy  \n",
    "        return loss\n",
    "    \n",
    "    def eval(self, preds, y):\n",
    "        assert len(preds) == len(y)\n",
    "        z = np.zeros(len(preds))\n",
    "        for i,p in enumerate(preds):\n",
    "            if p>0.5:\n",
    "                z[i] = 1\n",
    "        prec_score = precision_score(np.array(y), z)\n",
    "        rec_score = recall_score(np.array(y), z)\n",
    "        f1_score = (2 * prec_score * rec_score)/(prec_score + rec_score)\n",
    "        return (prec_score, rec_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = suggestionmodelBERTFinetuneSidewayLSTM().cuda()\n",
    "bertoptimizer2 = BertAdam(filter(lambda p: p.requires_grad,\n",
    "                                model2.parameters()), lr = 3e-5)\n",
    "#training\n",
    "if TRAIN:\n",
    "    train(model2, bertoptimizer2, data, val_data, 'BertFinetuneSidewayLstm')\n",
    "if EVAL:\n",
    "    model2.load_state_dict(torch.load('models/BertFinetuneSidewayLstm.bin'))\n",
    "    ModelList.append({'name':'BertFinetuneSidewayLstm', 'model':model2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL-3**\n",
    "\n",
    "*BERT + side-way lstm + 1d cnn*\n",
    "\n",
    "This model follows from the previous model. It also uses the side-way lstm. It takes the features from all the words and passes it though 1 conv1d layers. The final representations are obtained by taking a max across all words and an average across all words and combining the former and latter vector through concat. This is the first model in which I freezed the bert model. This model gives an f1-score on 0.85. This model takes 2 minutes per epoch on a GTX-1060 and this result can be achieved in about 30 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class suggestionmodelBERTFreezedSidewayLSTMCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(suggestionmodelBERTFreezedSidewayLSTMCNN, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('/home/aniket/Downloads/bert-base-uncased.tar.gz').cuda()\n",
    "        self.bert_model.eval()\n",
    "        self.lstm = nn.LSTM(768, 512)\n",
    "        self.cnn1d_1 = nn.Conv1d(512, 256, kernel_size = 3, padding = 1, bias = True)\n",
    "        #self.cnn1d_2 = nn.Conv1d(256, 64, kernel_size = 3, padding = 1, bias = True)\n",
    "        self.linear = nn.Linear(512, 1)\n",
    "        #weight = torch.tensor([3.])\n",
    "        self.loss_fn = nn.BCELoss(reduce = True)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        bert_avg_representaions = []\n",
    "        for instance in text:\n",
    "            instance = '[CLS] '+instance+' [SEP]'\n",
    "            tokenized_text = self.tokenizer.tokenize(instance)\n",
    "            if len(tokenized_text) > 512:\n",
    "                tokenized_text = tokenized_text[:512]\n",
    "            \n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens]).long().cuda()\n",
    "            segment_ids = torch.zeros(tokens_tensor.size()).long().cuda()\n",
    "            with torch.no_grad():\n",
    "                encoder_layers, _ = self.bert_model(tokens_tensor, segment_ids)\n",
    "            bert_representation = torch.squeeze(torch.stack(encoder_layers))\n",
    "            _, (bert_lstm_representation, _) = self.lstm(bert_representation)\n",
    "            #reshaping the data to be fit for 1d conv\n",
    "            bert_lstm_representation = torch.transpose(bert_lstm_representation, 2, 1)\n",
    "            #convolution\n",
    "            bert_lstm_representation = F.relu(self.cnn1d_1(bert_lstm_representation))\n",
    "            #bert_lstm_representation = F.relu(self.cnn1d_2(bert_lstm_representation))\n",
    "            bert_lstm_representation_max = torch.squeeze(torch.max(bert_lstm_representation, dim = 2)[0])\n",
    "            bert_lstm_representation_avg = torch.squeeze(torch.mean(bert_lstm_representation, \n",
    "                                                               dim = 2))\n",
    "            bert_lstm_representation = torch.cat((bert_lstm_representation_avg, bert_lstm_representation_max))\n",
    "            bert_avg_representaions.append(bert_lstm_representation)\n",
    "        \n",
    "        representation = torch.stack(bert_avg_representaions)\n",
    "        prediction = F.sigmoid(self.linear(representation))\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, preds, y):\n",
    "        y = torch.tensor(y).float().cuda()\n",
    "        preds = preds.reshape(-1)\n",
    "        #entropy calculation, acts as a regularizer\n",
    "        entropy = preds * torch.log(preds) + (1 - preds) * torch.log(preds)\n",
    "        entropy = - torch.sum(entropy)\n",
    "        loss = self.loss_fn(preds, y) + 0.05 * entropy  \n",
    "        return loss\n",
    "    \n",
    "    def eval(self, preds, y):\n",
    "        assert len(preds) == len(y)\n",
    "        z = np.zeros(len(preds))\n",
    "        for i,p in enumerate(preds):\n",
    "            if p>0.5:\n",
    "                z[i] = 1\n",
    "        prec_score = precision_score(np.array(y), z)\n",
    "        rec_score = recall_score(np.array(y), z)\n",
    "        f1_score = (2 * prec_score * rec_score)/(prec_score + rec_score)\n",
    "        return (prec_score, rec_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = suggestionmodelBERTFreezedSidewayLSTMCNN().cuda()\n",
    "bertoptimizer3 = BertAdam(filter(lambda p: p.requires_grad,\n",
    "                                model3.parameters()), lr = 3e-4)\n",
    "if TRAIN:\n",
    "    train(model3, bertoptimizer3, data, val_data, 'BERTFreezedSidewayLSTMCNN')\n",
    "if EVAL:\n",
    "    model3.load_state_dict(torch.load('models/BERTFreezedSidewayLSTMCNN.bin'))\n",
    "    ModelList.append({'name':'BERTFreezedSidewayLSTMCNN', 'model':model3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model-4**\n",
    "\n",
    "*frozen bert*\n",
    "\n",
    "This model is used to prove that the addition of side-way lstm and 1d convolutions are responsible for the rise in f1-score. This is a simple frozen bert model, with a fc layer on top. It gives an f1-score of about 0.73."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class suggestionmodelBERTFreezed(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(suggestionmodelBERTFreezed, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('/home/aniket/Downloads/bert-base-uncased.tar.gz').cuda()\n",
    "        self.bert_model.eval()\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.loss_fn = nn.BCELoss(reduce = True)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        bert_avg_representaions = []\n",
    "        for instance in text:\n",
    "            instance = '[CLS] '+instance+' [SEP]'\n",
    "            tokenized_text = self.tokenizer.tokenize(instance)\n",
    "            if len(tokenized_text) > 512:\n",
    "                tokenized_text = tokenized_text[:512]\n",
    "            \n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens]).long().cuda()\n",
    "            segment_ids = torch.zeros(tokens_tensor.size()).long().cuda()\n",
    "            with torch.no_grad():\n",
    "                encoder_layers, _ = self.bert_model(tokens_tensor, segment_ids)\n",
    "            bert_representation = encoder_layers[-1]\n",
    "            bert_representation = torch.mean(bert_representation, dim = 1)\n",
    "            bert_avg_representaions.append(bert_representation)\n",
    "        \n",
    "        representation = torch.stack(bert_avg_representaions)\n",
    "        prediction = F.sigmoid(self.linear(representation))\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, preds, y):\n",
    "        y = torch.tensor(y).float().cuda()\n",
    "        preds = preds.reshape(-1)\n",
    "        #entropy calculation, acts as a regularizer\n",
    "        entropy = preds * torch.log(preds) + (1 - preds) * torch.log(preds)\n",
    "        entropy = - torch.sum(entropy)\n",
    "        loss = self.loss_fn(preds, y) + 0.05 * entropy  \n",
    "        return loss\n",
    "    \n",
    "    def eval(self, preds, y):\n",
    "        assert len(preds) == len(y)\n",
    "        z = np.zeros(len(preds))\n",
    "        for i,p in enumerate(preds):\n",
    "            if p>0.5:\n",
    "                z[i] = 1\n",
    "        prec_score = precision_score(np.array(y), z)\n",
    "        rec_score = recall_score(np.array(y), z)\n",
    "        f1_score = (2 * prec_score * rec_score)/(prec_score + rec_score)\n",
    "        return (prec_score, rec_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = suggestionmodelBERTFreezed().cuda()\n",
    "bertoptimizer4 = BertAdam(filter(lambda p: p.requires_grad,\n",
    "                                model4.parameters()), lr = 3e-5)\n",
    "if TRAIN:\n",
    "    train(model4, bertoptimizer4, data, val_data, 'BertFreezed.bin')\n",
    "if EVAL:\n",
    "    model4.load_state_dict(torch.load('models/BertFreezed.bin'))\n",
    "    ModelList.append({'name':'BertFreezed', 'model':model4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model - 5**\n",
    "\n",
    "*BERT + SIDEWAY LSTM + LSTM*\n",
    "\n",
    "This model also uses sideway lstm. Instead of using 1d cnn across all words, here, I use an lstm across all words. the final hidden vector of the lstm is used for classification. This give an f1-score of about 0.81."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class suggestionmodelBERTFreezedSidewayLSTMLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(suggestionmodelBERTFreezedSidewayLSTMLSTM, self).__init__()\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.bert_model = BertModel.from_pretrained('/home/aniket/Downloads/bert-base-uncased.tar.gz').cuda()\n",
    "        self.bert_model.eval()\n",
    "        self.lstm = nn.LSTM(768, 1024)\n",
    "        self.lstm2 = nn.LSTM(1024, 1024)\n",
    "        self.linear = nn.Linear(1024, 1)\n",
    "        #weight = torch.tensor([3.])\n",
    "        self.loss_fn = nn.BCELoss(reduce = True)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        bert_avg_representaions = []\n",
    "        for instance in text:\n",
    "            instance = '[CLS] '+instance+' [SEP]'\n",
    "            tokenized_text = self.tokenizer.tokenize(instance)\n",
    "            if len(tokenized_text) > 512:\n",
    "                tokenized_text = tokenized_text[:512]\n",
    "            \n",
    "            indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "            tokens_tensor = torch.tensor([indexed_tokens]).long().cuda()\n",
    "            segment_ids = torch.zeros(tokens_tensor.size()).long().cuda()\n",
    "            with torch.no_grad():\n",
    "                encoder_layers, _ = self.bert_model(tokens_tensor, segment_ids)\n",
    "            bert_representation = torch.squeeze(torch.stack(encoder_layers))\n",
    "            _, (bert_lstm_representation, _) = self.lstm(bert_representation)\n",
    "            #reshaping the data to be fit for lstm\n",
    "            bert_lstm_representation = torch.transpose(bert_lstm_representation, 1, 0)\n",
    "            #lstm\n",
    "            _,(bert_lstm_representation, _) = self.lstm2(bert_lstm_representation)\n",
    "            \n",
    "            bert_lstm_representation = torch.squeeze(bert_lstm_representation)\n",
    "            bert_avg_representaions.append(bert_lstm_representation)\n",
    "        \n",
    "        representation = torch.stack(bert_avg_representaions)\n",
    "        prediction = F.sigmoid(self.linear(representation))\n",
    "        return prediction\n",
    "    \n",
    "    def loss(self, preds, y):\n",
    "        y = torch.tensor(y).float().cuda()\n",
    "        preds = preds.reshape(-1)\n",
    "        #entropy calculation, acts as a regularizer\n",
    "        entropy = preds * torch.log(preds) + (1 - preds) * torch.log(preds)\n",
    "        entropy = - torch.sum(entropy)\n",
    "        loss = self.loss_fn(preds, y) + 0.05 * entropy  \n",
    "        return loss\n",
    "    \n",
    "    def eval(self, preds, y):\n",
    "        assert len(preds) == len(y)\n",
    "        z = np.zeros(len(preds))\n",
    "        for i,p in enumerate(preds):\n",
    "            if p>0.5:\n",
    "                z[i] = 1\n",
    "        prec_score = precision_score(np.array(y), z)\n",
    "        rec_score = recall_score(np.array(y), z)\n",
    "        f1_score = (2 * prec_score * rec_score)/(prec_score + rec_score)\n",
    "        return (prec_score, rec_score, f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = suggestionmodelBERTFreezedSidewayLSTMLSTM().cuda()\n",
    "bertoptimizer5 = BertAdam(filter(lambda p: p.requires_grad,\n",
    "                                model5.parameters()), lr = 3e-5)\n",
    "if TRAIN:\n",
    "    train(model5, bertoptimizer5, data, val_data, 'BERTFreezedSidewayLSTMLSTM')\n",
    "if EVAL:\n",
    "    model5.load_state_dict(torch.load('models/BERTFreezedSidewayLSTMLSTM.bin'))\n",
    "    ModelList.append({'name':'BERTFreezedSidewayLSTMLSTM', 'model':model5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below block gives the f1-score for all the trained models on the validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation\n",
    "def evaluation(val_data, model_dict):\n",
    "    model = model_dict['model']\n",
    "    model_name = model_dict['name']\n",
    "    val_batched_text, val_batched_labels = batchify(val_data)\n",
    "    #looping through the val data\n",
    "    test_preds = []\n",
    "    test_targets = []\n",
    "    for i in range(len(val_batched_labels)):\n",
    "        x = val_batched_text[i]\n",
    "        y = val_batched_labels[i]\n",
    "        preds = model(x)\n",
    "        test_preds.append(preds.data.cpu().numpy())\n",
    "        test_targets.append(y)\n",
    "\n",
    "    test_preds = [k[i] for k in test_preds for i in range(k.shape[0])]\n",
    "    test_targets = [i for item in test_targets for i in item]\n",
    "    test_eval = model.eval(test_preds, test_targets)\n",
    "    print('model:'+str(model_name)+' precision:'+str(test_eval[0])+' recall:'+\n",
    "          str(test_eval[1])+' f1:'+str(test_eval[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.local/lib/python3.5/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:BertFinetune precision:0.8652173913043478 recall:0.6722972972972973 f1:0.7566539923954373\n",
      "model:BertFinetuneSidewayLstm precision:0.8299319727891157 recall:0.8243243243243243 f1:0.8271186440677967\n",
      "model:BERTFreezedSidewayLSTMCNN precision:0.753315649867374 recall:0.9594594594594594 f1:0.8439821693907875\n",
      "model:BertFreezed precision:0.5914634146341463 recall:0.9831081081081081 f1:0.7385786802030455\n",
      "model:BERTFreezedSidewayLSTMLSTM precision:0.825 recall:0.7804054054054054 f1:0.8020833333333333\n"
     ]
    }
   ],
   "source": [
    "for model in ModelList:\n",
    "    evaluation(val_data, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing final result to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csvwrite(test_data, test_data_csv, model):\n",
    "    test_batched_text, test_batched_labels = batchify(test_data)\n",
    "    #looping through the val data\n",
    "    test_preds = []\n",
    "    for i in range(len(test_batched_labels)):\n",
    "        x = test_batched_text[i]\n",
    "        y = test_batched_labels[i]\n",
    "        preds = model(x)\n",
    "        test_preds.append(preds.data.cpu().numpy())\n",
    "    test_preds = [k[i] for k in test_preds for i in range(k.shape[0])]\n",
    "    assert len(test_preds) == len(test_data_csv)\n",
    "    print(len(test_preds))\n",
    "    for i, preds in enumerate(test_preds):\n",
    "        if preds > 0.5:\n",
    "            test_data_csv[i][2] = 1\n",
    "        else:\n",
    "            test_data_csv[i][2] = 0\n",
    "    with open('AniketDidolkar.csv', 'w') as writeFile:\n",
    "        writer = csv.writer(writeFile)\n",
    "        writer.writerows(test_data_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniket/.local/lib/python3.5/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "833\n"
     ]
    }
   ],
   "source": [
    "csvwrite(test_data, test_data_csv, ModelList[2]['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
